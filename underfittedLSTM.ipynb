{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9149877,"sourceType":"datasetVersion","datasetId":5526909}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rubinr12/underfittedlstm-ipynb?scriptVersionId=192164380\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input, SimpleRNN, GRU, BatchNormalization, Conv1D\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Precision, Recall","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:48:35.341698Z","iopub.execute_input":"2024-08-11T07:48:35.342067Z","iopub.status.idle":"2024-08-11T07:48:35.349087Z","shell.execute_reply.started":"2024-08-11T07:48:35.342037Z","shell.execute_reply":"2024-08-11T07:48:35.348027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_pipeline(file_path, window_size=20):\n    df = pd.read_excel(file_path)\n\n    # Drop the 'job_id' column\n    df = df.drop(columns=['job_id'])\n    \n    # Create sliding windows\n    def create_sliding_windows(data, window_size):\n        X = []\n        y = []\n        num_rows = len(data)\n        \n        if num_rows <= window_size:\n            raise ValueError(\"Data length must be greater than the window size.\")\n        \n        for i in range(len(data) - window_size):\n            X.append(data[i:i + window_size, :-1])  \n            y.append(data[i + window_size, -1])     \n    \n        return np.array(X), np.array(y)\n    \n    data = df.values\n    X, y = create_sliding_windows(data, 20)\n    \n    # Split sliding windows into training, validation, and test sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test","metadata":{"execution":{"iopub.status.busy":"2024-08-11T08:59:51.07353Z","iopub.execute_input":"2024-08-11T08:59:51.073931Z","iopub.status.idle":"2024-08-11T08:59:51.082781Z","shell.execute_reply.started":"2024-08-11T08:59:51.073903Z","shell.execute_reply":"2024-08-11T08:59:51.081752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/dataset/train.xlsx'\nX_train, X_val, X_test, y_train, y_val, y_test = data_pipeline(file_path, window_size=20)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T08:59:56.84394Z","iopub.execute_input":"2024-08-11T08:59:56.844314Z","iopub.status.idle":"2024-08-11T09:00:20.60807Z","shell.execute_reply.started":"2024-08-11T08:59:56.844284Z","shell.execute_reply":"2024-08-11T09:00:20.607278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metrics_new = [\n#     keras.metrics.Precision(name=\"precision\"),\n#     keras.metrics.Recall(name=\"recall\"),\n# ]\n\n# window_size = 20\n# learning_rate = 0.0001\n# optimizer = Adam(learning_rate=learning_rate)\n\n# model1 = Sequential()\n# model1.add(Input(shape=(window_size, X_train.shape[2])))\n# model1.add(LSTM(units = 128, activation='tanh',return_sequences= True))\n# model1.add(BatchNormalization())\n# model1.add(Dropout(0.2))\n# model1.add(LSTM(units = 64, activation='tanh',return_sequences= True))\n# model1.add(BatchNormalization())\n# model1.add(Dropout(0.2))\n# model1.add(LSTM(units = 64, activation='tanh'))\n# model1.add(BatchNormalization())\n# model1.add(Dropout(0.2))\n# model1.add(Dense(1, activation='sigmoid'))\n# model1.compile(optimizer=optimizer, loss='BinaryCrossentropy', metrics=metrics_new)\n# model1.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:21:25.809816Z","iopub.execute_input":"2024-08-11T07:21:25.810197Z","iopub.status.idle":"2024-08-11T07:21:26.46179Z","shell.execute_reply.started":"2024-08-11T07:21:25.810169Z","shell.execute_reply":"2024-08-11T07:21:26.460883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n\n# Set up early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model\nhistory = model1.fit(X_train, y_train,\n                     epochs=125,\n                     batch_size=128,\n                     validation_data=(X_val, y_val),\n                     callbacks=[early_stopping],\n                     verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:22:40.00263Z","iopub.execute_input":"2024-08-11T07:22:40.003446Z","iopub.status.idle":"2024-08-11T07:25:26.995659Z","shell.execute_reply.started":"2024-08-11T07:22:40.003413Z","shell.execute_reply":"2024-08-11T07:25:26.994659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Version 2\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, BatchNormalization, Dropout, Dense, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.regularizers import l2\n\n# Define metrics\nmetrics_new = [\n    keras.metrics.Precision(name=\"precision\"),\n    keras.metrics.Recall(name=\"recall\"),\n]\n\n# Hyperparameters\nwindow_size = 20\nlearning_rate = 0.00001\noptimizer = Adam(learning_rate=learning_rate)\ndropout_rate = 0.3  # Increased dropout for regularization\nl2_reg = 0.01  # L2 regularization\n\n# Model architecture\nmodel1 = Sequential()\nmodel1.add(Input(shape=(window_size, X_train.shape[2])))\nmodel1.add(LSTM(units=64, activation='tanh', return_sequences=True))\nmodel1.add(BatchNormalization())\nmodel1.add(Dropout(dropout_rate))\nmodel1.add(LSTM(units=32, activation='tanh', return_sequences=True))\nmodel1.add(BatchNormalization())\nmodel1.add(Dropout(dropout_rate))\nmodel1.add(LSTM(units=32, activation='tanh'))\nmodel1.add(BatchNormalization())\nmodel1.add(Dropout(dropout_rate))\nmodel1.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_reg)))\nmodel1.compile(optimizer=optimizer, loss='BinaryCrossentropy', metrics=metrics_new)\nmodel1.summary()\n\n# Learning rate scheduler\ndef lr_schedule(epoch, lr):\n    if epoch > 10:\n        lr = lr * 0.5\n    return lr\n\n# Callbacks\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\n# Train the model\nhistory = model1.fit(X_train, y_train,\n                     epochs=150,\n                     batch_size=64,  # Reduced batch size\n                     validation_data=(X_val, y_val),\n                     callbacks=[early_stopping, reduce_lr, lr_scheduler],\n                     verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:55:40.641963Z","iopub.execute_input":"2024-08-11T07:55:40.642368Z","iopub.status.idle":"2024-08-11T08:05:35.766624Z","shell.execute_reply.started":"2024-08-11T07:55:40.642333Z","shell.execute_reply":"2024-08-11T08:05:35.765714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results = model1.evaluate(X_test, y_test, verbose=1)\n\ntest_loss = evaluation_results[0]\ntest_precision = evaluation_results[1]\ntest_recall = evaluation_results[2]\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Precision: {test_precision}\")\nprint(f\"Test Recall: {test_recall}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T08:28:02.725511Z","iopub.execute_input":"2024-08-11T08:28:02.725891Z","iopub.status.idle":"2024-08-11T08:28:03.838219Z","shell.execute_reply.started":"2024-08-11T08:28:02.725862Z","shell.execute_reply":"2024-08-11T08:28:03.837444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-11T08:28:08.079957Z","iopub.execute_input":"2024-08-11T08:28:08.080342Z","iopub.status.idle":"2024-08-11T08:28:08.275173Z","shell.execute_reply.started":"2024-08-11T08:28:08.080314Z","shell.execute_reply":"2024-08-11T08:28:08.274302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n\n# Make predictions\npredictions_new = model1.predict(X_new)\n\n# Threshold for binary classification\ncheck_value = 0.5\npredictions_01 = (predictions_new > check_value).astype(int)\ny_new_binary = (y_new > check_value).astype(int)\n\n# Calculate precision, recall, F1 score, and confusion matrix\nprecision = precision_score(y_new_binary, predictions_01, average='macro')\nrecall = recall_score(y_new_binary, predictions_01, average='macro')\nf1 = f1_score(y_new_binary, predictions_01, average='macro')\nconf_matrix = confusion_matrix(y_new_binary, predictions_01)\n\n# Output the results\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 Score: {f1}')\nprint(f'Confusion Matrix:\\n{conf_matrix}')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T07:54:27.00802Z","iopub.execute_input":"2024-08-11T07:54:27.008747Z","iopub.status.idle":"2024-08-11T07:54:36.41052Z","shell.execute_reply.started":"2024-08-11T07:54:27.00871Z","shell.execute_reply":"2024-08-11T07:54:36.409454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Version3\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, BatchNormalization, Dropout, Dense, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.regularizers import l2\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-08-11T08:58:14.509026Z","iopub.execute_input":"2024-08-11T08:58:14.509456Z","iopub.status.idle":"2024-08-11T08:58:14.515452Z","shell.execute_reply.started":"2024-08-11T08:58:14.509423Z","shell.execute_reply":"2024-08-11T08:58:14.51421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameters\nwindow_size = 20\nlearning_rate = 0.001\ndropout_rate = 0.3\nl2_reg = 0.01\nbatch_size = 64\nepochs = 150","metadata":{"execution":{"iopub.status.busy":"2024-08-11T09:00:33.763131Z","iopub.execute_input":"2024-08-11T09:00:33.76375Z","iopub.status.idle":"2024-08-11T09:00:33.768444Z","shell.execute_reply.started":"2024-08-11T09:00:33.763718Z","shell.execute_reply":"2024-08-11T09:00:33.767518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define model architecture\nmodel = Sequential()\nmodel.add(Input(shape=(window_size, X_train.shape[2])))\nmodel.add(LSTM(units=64, activation='tanh', return_sequences=True, kernel_regularizer=l2(l2_reg)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(dropout_rate))\nmodel.add(LSTM(units=32, activation='tanh', return_sequences=True, kernel_regularizer=l2(l2_reg)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(dropout_rate))\nmodel.add(LSTM(units=32, activation='tanh', kernel_regularizer=l2(l2_reg)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(dropout_rate))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T09:00:36.947388Z","iopub.execute_input":"2024-08-11T09:00:36.948082Z","iopub.status.idle":"2024-08-11T09:00:37.146153Z","shell.execute_reply.started":"2024-08-11T09:00:36.948051Z","shell.execute_reply":"2024-08-11T09:00:37.144869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define learning rate schedule\ndef lr_schedule(epoch, lr):\n    if epoch > 10:\n        lr = lr * 0.5\n    return lr","metadata":{"execution":{"iopub.status.busy":"2024-08-11T09:00:39.166482Z","iopub.execute_input":"2024-08-11T09:00:39.167909Z","iopub.status.idle":"2024-08-11T09:00:39.17305Z","shell.execute_reply.started":"2024-08-11T09:00:39.167851Z","shell.execute_reply":"2024-08-11T09:00:39.172102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define callbacks\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=learning_rate), loss='BinaryCrossentropy', metrics=['accuracy', 'precision', 'recall'])\n\n# Train model\nhistory = model.fit(X_train, y_train,\n                     epochs=epochs,\n                     batch_size=batch_size,\n                     validation_data=(X_val, y_val),\n                     callbacks=[early_stopping, reduce_lr, lr_scheduler],\n                     verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T09:00:49.291531Z","iopub.execute_input":"2024-08-11T09:00:49.292157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model\ndf_new = pd.read_csv('job2.csv')\ndf_new = df_new.drop(columns=['job_id','Latitude', 'Longitude'])\ndef create_sliding_windows_1(data, window_size):\n    X = []\n    y = []\n    data1=data.values\n    for i in range(len(data) - window_size):\n        X.append(data1[i:i + window_size, :-1])  \n        y.append(data1[i + window_size, -1])     \n    \n    return np.array(X), np.array(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_new, y_new = create_sliding_windows_1(df_new, 20)\n\npredictions_new = model1.predict(X_new)\n\ncheck_value = 0.5\npredictions_01 = (predictions_new > check_value).astype(int)\ny_new_binary = (y_new > check_value).astype(int)\n\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n\nprecision = precision_score(y_new, predictions_01, average='macro')\nrecall = recall_score(y_new, predictions_01, average='macro')\nf1 = f1_score(y_new, predictions_01, average='macro')\nconf_matrix = confusion_matrix(y_new, predictions_01)\n\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 Score: {f1}')\nprint(f'Confusion Matrix:\\n{conf_matrix}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###################################################################\nevaluation_results = model.evaluate(X_test, y_test, verbose=1)\n\n# Make predictions\npredictions_new = model.predict(X_test)\n\n# Threshold for binary classification\ncheck_value = 0.5\npredictions_01 = (predictions_new > check_value).astype(int)\ny_new_binary = (y_test > check_value).astype(int)\n\n# Calculate precision, recall, F1 score, and confusion matrix\nprecision = precision_score(y_new_binary, predictions_01, average='macro')\nrecall = recall_score(y_new_binary, predictions_01, average='macro')\nf1 = f1_score(y_new_binary, predictions_01, average='macro')\nconf_matrix = confusion_matrix(y_new_binary, predictions_01)\n\n# Output the results\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 Score: {f1}')\nprint(f'Confusion Matrix:\\n{conf_matrix}')","metadata":{},"execution_count":null,"outputs":[]}]}